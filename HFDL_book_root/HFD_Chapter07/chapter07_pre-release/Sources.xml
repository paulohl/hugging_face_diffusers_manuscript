<?xml version="1.0"?>
<b:Sources SelectedStyle="" xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" xmlns="http://schemas.openxmlformats.org/officeDocument/2006/bibliography"><b:Source>
		<b:Year>2017</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>Attention Is All You Need</b:Title>
		<b:Tag>Vaswani2017</b:Tag>
		<b:BookTitle>Advances in Neural Information Processing Systems</b:BookTitle>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Vaswani</b:Last>
						<b:First>A.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:ConferenceName>Advances in Neural Information Processing Systems</b:ConferenceName>
		<b:RefOrder>11</b:RefOrder></b:Source><b:Source><b:Tag>Sut18</b:Tag><b:SourceType>Book</b:SourceType><b:Guid>{C2B89B5A-E064-4A46-8BAD-79D7210F94E4}</b:Guid><b:Author><b:Author><b:NameList><b:Person><b:Last>Sutton</b:Last><b:First>R.</b:First><b:Middle>S.</b:Middle></b:Person><b:Person><b:Last>Barto</b:Last><b:First>A.</b:First><b:Middle>G.</b:Middle></b:Person></b:NameList></b:Author></b:Author><b:Title>Reinforcement learning: An introduction</b:Title><b:Year>2018</b:Year><b:Publisher>MIT Press</b:Publisher><b:RefOrder>2</b:RefOrder></b:Source><b:Source><b:Tag>Sut13</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{3AD3389B-D019-46CE-ABF6-A3EDA6EC8226}</b:Guid><b:Title>On the importance of initialization and momentum in deep learning</b:Title><b:Year>2013</b:Year><b:Pages>1139–1147</b:Pages><b:YearAccessed>2025</b:YearAccessed><b:MonthAccessed>January</b:MonthAccessed><b:DayAccessed>07</b:DayAccessed><b:URL> http://proceedings.mlr.press/v28/sutskever13.html</b:URL><b:Author><b:Author><b:NameList><b:Person><b:Last>Sutskever</b:Last><b:First>I.</b:First></b:Person><b:Person><b:Last>Martens</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Dahl</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Hinton</b:Last><b:First>G.</b:First></b:Person></b:NameList></b:Author></b:Author><b:ConferenceName>Proceedings of the 30th International Conference on Machine Learning (ICML)</b:ConferenceName><b:RefOrder>8</b:RefOrder></b:Source><b:Source>
		<b:Year>2014</b:Year>
		<b:Volume>15</b:Volume>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</b:Title>
		<b:Tag>Srivastava2014</b:Tag>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Srivastava</b:Last>
						<b:First>N.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:Pages>1929, 1958</b:Pages>
		<b:JournalName>Journal of Machine Learning Research</b:JournalName>
		<b:RefOrder>12</b:RefOrder></b:Source><b:Source>
		<b:Year>2017</b:Year>
		<b:BIBTEX_Entry>inproceedings</b:BIBTEX_Entry>
		<b:SourceType>ConferenceProceedings</b:SourceType>
		<b:Title>Cyclical Learning Rates for Training Neural Networks</b:Title>
		<b:Tag>Smith2017</b:Tag>
		<b:BookTitle>2017 IEEE Winter Conference on Applications of Computer Vision (WACV</b:BookTitle>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Smith</b:Last>
						<b:Middle>N.</b:Middle>
						<b:First>L.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:ConferenceName>2017 IEEE Winter Conference on Applications of Computer Vision (WACV</b:ConferenceName>
		<b:RefOrder>4</b:RefOrder></b:Source><b:Source>
		<b:Year>2019</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:Comments>Preprint arXiv:1910.01108.</b:Comments>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</b:Title>
		<b:Tag>Sanh2019</b:Tag>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Sanh</b:Last>
						<b:First>V.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:RefOrder>13</b:RefOrder></b:Source><b:Source><b:Tag>Nic21</b:Tag><b:SourceType>ConferenceProceedings</b:SourceType><b:Guid>{06171672-108D-4A13-A26D-410BB276555C}</b:Guid><b:Title>Improved Denoising Diffusion Probabilistic Models</b:Title><b:Year>2021</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Nichol</b:Last><b:First>A.</b:First><b:Middle>Q.</b:Middle></b:Person><b:Person><b:Last>Dhariwal</b:Last><b:First>P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:ConferenceName>International Conference on Machine Learning (ICML).</b:ConferenceName><b:URL> https://arxiv.org/abs/2102.09672/</b:URL><b:RefOrder>10</b:RefOrder></b:Source><b:Source>
		<b:Year>2016</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:Comments>Preprint arXiv:1608.03983.</b:Comments>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>SGDR: Stochastic Gradient Descent with Warm Restarts</b:Title>
		<b:Tag>Loshchilov2016</b:Tag>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Loshchilov</b:Last>
						<b:First>I.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Hutter</b:Last>
						<b:First>F.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:RefOrder>7</b:RefOrder></b:Source><b:Source>
		<b:Year>2015</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:Comments>Preprint arXiv:1412.6980.</b:Comments>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>Adam: A Method for Stochastic Optimization</b:Title>
		<b:Tag>Kingma2015</b:Tag>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Kingma</b:Last>
						<b:Middle>P.</b:Middle>
						<b:First>D.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Ba</b:Last>
						<b:First>J.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:RefOrder>9</b:RefOrder></b:Source><b:Source><b:Tag>HoJ20</b:Tag><b:SourceType>JournalArticle</b:SourceType><b:Guid>{3F8069B8-3D7E-4E74-A64B-EF07DAF41415}</b:Guid><b:Title>Denoising Diffusion Probabilistic Models</b:Title><b:Year>2020</b:Year><b:Publisher>NeurIPS</b:Publisher><b:Author><b:Author><b:NameList><b:Person><b:Last>Ho</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Jain</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Abbeel</b:Last><b:First>P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:JournalName>Advances in Neural Information Processing Systems</b:JournalName><b:Pages>6840–6851</b:Pages><b:Volume>33</b:Volume><b:URL>https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf/</b:URL><b:RefOrder>5</b:RefOrder></b:Source><b:Source><b:Tag>Goy17</b:Tag><b:SourceType>ArticleInAPeriodical</b:SourceType><b:Guid>{C2E2DEA5-A20C-46CE-9715-01868AF4C339}</b:Guid><b:Title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</b:Title><b:Year>2017</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Goyal</b:Last><b:First>P.</b:First></b:Person><b:Person><b:Last>Dollár</b:Last><b:First>P.</b:First></b:Person><b:Person><b:Last>Girshick</b:Last><b:First>R.</b:First></b:Person><b:Person><b:Last>Noordhuis</b:Last><b:First>P.</b:First></b:Person><b:Person><b:Last>Wesolowski</b:Last><b:First>L.</b:First></b:Person><b:Person><b:Last>Kyrola</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Tulloch</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Jia</b:Last><b:First>Y.</b:First></b:Person><b:Person><b:Last>He</b:Last><b:First>K.</b:First></b:Person></b:NameList></b:Author></b:Author><b:PeriodicalTitle>arXiv preprint</b:PeriodicalTitle><b:Edition>/</b:Edition><b:StandardNumber>arXiv:1706.02677</b:StandardNumber><b:URL>https://arxiv.org/abs/1706.02677/</b:URL><b:RefOrder>6</b:RefOrder></b:Source><b:Source>
		<b:Year>2016</b:Year>
		<b:BIBTEX_Entry>book</b:BIBTEX_Entry>
		<b:SourceType>Book</b:SourceType>
		<b:Title>Deep Learning</b:Title>
		<b:Publisher>MIT Press</b:Publisher>
		<b:Tag>Goodfellow2016</b:Tag>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Goodfellow</b:Last>
						<b:First>I.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Bengio</b:Last>
						<b:First>Y.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Courville</b:Last>
						<b:First>A.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:RefOrder>1</b:RefOrder></b:Source><b:Source>
		<b:BIBTEX_Entry>inproceedings</b:BIBTEX_Entry>
		<b:SourceType>ConferenceProceedings</b:SourceType>
		<b:Title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</b:Title>
		<b:Tag>Devlin2019</b:Tag>
		<b:BookTitle>Proceedings of NAACL-HLT</b:BookTitle>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Devlin</b:Last>
						<b:First>J.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Chang</b:Last>
						<b:Middle>W.</b:Middle>
						<b:First>M.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Lee</b:Last>
						<b:First>K.</b:First>
					</b:Person>
					<b:Person>
						<b:Last>Toutanova</b:Last>
						<b:First>K.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:Year>2019</b:Year>
		<b:ConferenceName>Proceedings of NAACL-HLT</b:ConferenceName>
		<b:RefOrder>14</b:RefOrder></b:Source><b:Source>
		<b:Year>2020</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>Legal-BERT: The Muppets straight out of Law School</b:Title>
		<b:Publisher>EMNLP</b:Publisher>
		<b:Tag>Chalkidis2020</b:Tag>
		<b:BookTitle>Findings of the Association for Computational</b:BookTitle>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Chalkidis</b:Last>
						<b:First>I.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:ConferenceName>Findings of the Association for Computational</b:ConferenceName>
		<b:City>Linguistics</b:City>
		<b:RefOrder>15</b:RefOrder></b:Source><b:Source>
		<b:Year>2020</b:Year>
		<b:BIBTEX_Entry>article</b:BIBTEX_Entry>
		<b:SourceType>JournalArticle</b:SourceType>
		<b:Title>Language Models are Few-Shot Learners</b:Title>
		<b:Tag>Brown2020</b:Tag>
		<b:BookTitle>Advances in Neural Information Processing Systems</b:BookTitle>
		<b:Author>
			<b:Author>
				<b:NameList>
					<b:Person>
						<b:Last>Brown</b:Last>
						<b:Middle>B.</b:Middle>
						<b:First>T.</b:First>
					</b:Person>
				</b:NameList>
			</b:Author>
		</b:Author>
		<b:ConferenceName>Advances in Neural Information Processing Systems</b:ConferenceName>
		<b:RefOrder>16</b:RefOrder></b:Source><b:Source><b:Tag>Ben12</b:Tag><b:SourceType>BookSection</b:SourceType><b:Guid>{13642999-53F7-4BEB-A759-CB7FD905FCB0}</b:Guid><b:Title>Practical Recommendations for Gradient-Based Training of Deep Architectures</b:Title><b:Year>2012</b:Year><b:City>Berlin</b:City><b:Author><b:Author><b:NameList><b:Person><b:Last>Bengio</b:Last><b:First>Y.</b:First></b:Person></b:NameList></b:Author><b:BookAuthor><b:NameList><b:Person><b:Last>Bengio</b:Last><b:First>Y.</b:First></b:Person></b:NameList></b:BookAuthor></b:Author><b:JournalName>Practical Recommendations for Gradient-Based Training of Deep Architectures</b:JournalName><b:Pages>437–478</b:Pages><b:Publisher>Springer</b:Publisher><b:YearAccessed>2025</b:YearAccessed><b:MonthAccessed>January</b:MonthAccessed><b:DayAccessed>07</b:DayAccessed><b:BookTitle>Neural Networks: Tricks of the Trade</b:BookTitle><b:StateProvince>Heidelberg</b:StateProvince><b:Edition>2nd</b:Edition><b:URL>https://link.springer.com/chapter/10.1007/978-3-642-35289-8_26</b:URL><b:RefOrder>3</b:RefOrder></b:Source></b:Sources>
