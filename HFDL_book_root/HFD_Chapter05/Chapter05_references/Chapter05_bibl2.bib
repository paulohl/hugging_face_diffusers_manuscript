@Article{Anderson2021a,
  author  = {Anderson, B. and McGrew, D.},
  journal = {Journal of Cybersecurity and Privacy},
  title   = {Machine Learning for Threat Hunting: Techniques and Tools},
  year    = {2021},
  number  = {2},
  pages   = {134--156},
  volume  = {1},
}

@InProceedings{Lee2020b,
  author    = {Lee, C. and Kim, J.},
  booktitle = {Proceedings of the International Conference on Digital Forensics},
  title     = {AI-Driven Digital Forensics: Challenges and Approaches},
  year      = {2020},
  pages     = {98--112},
}

@Article{Sommer2010a,
  author    = {Sommer, Robin and Paxson, Vern},
  journal   = {Journal of Information Security},
  title     = {Outside the Closed World: On Using Machine Learning for Network Intrusion Detection},
  year      = {2010},
  pages     = {305--316},
  abstract  = {Anomaly based approaches in network intrusion detection suffer from evaluation, comparison and deployment which originate from the scarcity of adequate publicly available network trace datasets. Also, publicly available datasets are either outdated or generated in a controlled environment. Due to the ubiquity of cloud computing environments in commercial and government internet services, there is a need to assess the impacts of network attacks in cloud data centers. To the best of our knowledge, there is no publicly available dataset which captures the normal and anomalous network traces in the interactions between cloud users and cloud data centers. In this paper, we present an experimental platform designed to represent a practical interaction between cloud users and cloud services and collect network traces resulting from this interaction to conduct anomaly detection. We use Amazon web services (AWS) platform for conducting our experiments.},
  booktitle = {2010 IEEE Symposium on Security and Privacy},
  doi       = {10.1109/sp.2010.25},
  keywords  = {Machine learning;Intrusion detection;Computer security;National security;anomaly detection},
  publisher = {IEEE},
  url       = {https://api.semanticscholar.org/CorpusID:206578669},
}

@Article{Han2022a,
  author  = {Han, J. and Lee, Y.},
  journal = {Computers and Security},
  title   = {Threat Intelligence Integration in Security Information and Event Management Systems},
  year    = {2022},
  pages   = {102535},
  volume  = {112},
}

@Book{Stallings2017a,
  author    = {Stallings, William},
  publisher = {Pearson Education},
  title     = {Cryptography and network security},
  year      = {2017},
  address   = {Boston},
  edition   = {7},
  isbn      = {9780134444284},
  month     = sep,
  pagetotal = {748},
  ppn_gvk   = {1618773178},
  subtitle  = {Principles and practice},
  url       = {https://www.semanticscholar.org/paper/d7e732b3c9f8ab14f0548527abf13ad8b8cd093c},
}

@Article{Sharafaldin2018a,
  author    = {Sharafaldin, Iman and Habibi Lashkari, Arash and Ghorbani, Ali A.},
  journal   = {Proceedings of the ICISSP 2018},
  title     = {Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization},
  year      = {2018},
  month     = {January},
  note      = {Enthält: 2nd International Workshop on Formal Methods for Security Engineering (ForSE 2018)},
  pages     = {108-116},
  volume    = {1},
  abstract  = {With exponential growth in the size of computer networks and developed applications, the significant increasing of the potential damage that can be caused by launching attacks is becoming obvious. Meanwhile, Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are one of the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of adequate dataset, anomaly-based approaches in intrusion detection systems are suffering from accurate deployment, analysis and evaluation. There exist a number of such datasets such as DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their proposed intrusion detection and intrusion prevention approaches. Based on our study over eleven available datasets since 1998, many such datasets are out of date and unreliable to use. Some of these datasets suffer from lack of traffic diversity and volumes, some of them do not cover the variety of attacks, while others anonymized packet information and payload which cannot reflect the current trends, or they lack feature set and metadata. This paper produces a reliable dataset that contains benign and seven common attack network flows, which meets real world criteria and is publicly available. Consequently, the paper evaluates the performance of a comprehensive set of network traffic features and machine learning algorithms to indicate the best set of features for detecting the certain attack categories.},
  address   = {[Setúbal, Portugal]},
  booktitle = {Proceedings of the 4th International Conference on Information Systems Security and Privacy},
  doi       = {10.5220/0006639801080116},
  editor    = {Paolo Mori and Steven Furnell and Olivier Camp},
  isbn      = {9789897582820},
  keywords  = {used CAIDA data},
  pagetotal = {608},
  ppn_gvk   = {1011045419},
  publisher = {SCITEPRESS - Science and Technology Publications},
  subtitle  = {International Conference on Information Systems Security and Privacy},
}

@Article{Devlin2018a,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2018},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.04805},
  eprint        = {1810.04805},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1810.04805},
}

@Article{Zhang2015a,
  author    = {Zhang, X. and Zhao, J. and LeCun, Y.},
  journal   = {Advances in Neural Information Processing Systems},
  title     = {Character-level Convolutional Networks for Text Classificatio},
  year      = {2015},
  pages     = {649--657},
  volume    = {28},
  publisher = {NeurIPS},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
}

@Article{Sanh2019a,
  author    = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  title     = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year      = {2019},
  note      = {arXiv preprint arXiv:1910.01108},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1910.01108},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/1910.01108/},
}

@Article{Srivastava2014a,
  author     = {Srivastava, N. and Hinton, G. and Krizhevsky, A. and Sutskever, I. and Salakhutdinov, R.},
  journal    = {Journal of Machine Learning Research},
  title      = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year       = {2014},
  issn       = {1532-4435},
  month      = jan,
  number     = {1},
  pages      = {1929--1958},
  volume     = {15},
  abstract   = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  issue      = {56},
  issue_date = {January 2014},
  keywords   = {deep learning, model combination, neural networks, regularization},
  publisher  = {JMLR.org},
  url        = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@Article{Raffel2020a,
  author     = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  journal    = {J. Mach. Learn. Res.},
  title      = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  year       = {2020},
  issn       = {1532-4435},
  month      = jan,
  note       = {Preprint arXiv:1910.10683.},
  number     = {1},
  volume     = {21},
  abstract   = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  articleno  = {140},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1910.10683},
  issue_date = {January 2020},
  keywords   = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  numpages   = {67},
  publisher  = {JMLR.org},
  url        = {http://arxiv.org/abs/1910.10683},
}

@InProceedings{Pires2019a,
  author        = {Telmo Pires and Eva Schlinger and Dan Garrette},
  title         = {How multilingual is Multilingual BERT?},
  year          = {2019},
  organization  = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages         = {4996--5001},
  publisher     = {Association for Computational Linguistics},
  abstract      = {In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/arXiv.1906.01502},
  eprint        = {1906.01502},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1906.01502},
}

@Article{Chalkidis2020a,
  author    = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  title     = {LEGAL-BERT: The Muppets straight out of law school},
  year      = {2020},
  note      = {arXiv preprint arXiv:2010.02559/},
  pages     = {2898--2904},
  abstract  = {BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  doi       = {10.18653/v1/2020.findings-emnlp.261},
  publisher = {Association for Computational Linguistics},
  url       = {https://arxiv.org/abs/2010.02559/},
}

@InProceedings{Ruder2019a,
  author       = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle    = {Proceedings of the 2019 Conference of the North},
  title        = {Transfer Learning in Natural Language Processing},
  year         = {2019},
  organization = {Proceedings of NAACL 2019 Tutorial},
  publisher    = {Association for Computational Linguistics},
  doi          = {10.18653/v1/n19-5004},
  url          = {https://aclanthology.org/N19-5004/},
}

@Article{Howard2018a,
  author        = {Howard, Jeremy and Ruder, Sebastian},
  journal       = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title         = {Universal Language Model Fine-tuning for Text Classification},
  year          = {2018},
  note          = {Preprint arXiv:1801.06146.},
  pages         = {328--339},
  abstract      = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.18653/v1/p18-1031},
  eprint        = {1801.06146},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {Association for Computational Linguistics},
  url           = {https://arxiv.org/abs/1801.06146},
}

@Article{Conneau2020a,
  author        = {Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzman and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
  title         = {Unsupervised Cross-lingual Representation Learning at Scale},
  year          = {2020},
  abstract      = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.1911.02116},
  eprint        = {1911.02116},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1911.02116},
}

@Article{Brown2020a,
  author        = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {Language Models are Few-Shot Learners},
  year          = {2020},
  pages         = {1877--1901},
  volume        = {33},
  archiveprefix = {arXiv},
  booktitle     = {Advances in Neural Information Processing Systems},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2005.14165},
  editor        = {H. Larochelle and M. Ranzato and R. Hadsell and M .F. Balcan and H. Lin},
  eprint        = {2005.14165},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher     = {Curran Associates, Inc.},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
}

@Article{Chen2022a,
  author        = {Chen, Zhuangbin and Liu, Jinyang and Gu, Wenwei and Su, Yuxin and Lyu, Michael R.},
  journal       = {ACM Computing Surveys},
  title         = {Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection},
  year          = {2022},
  issn          = {0360-0300},
  pages         = {1285--1298},
  volume        = {482},
  abstract      = {Logs have been an imperative resource to ensure the reliability and continuity of many software systems, especially large-scale distributed systems. They faithfully record runtime information to facilitate system troubleshooting and behavior understanding. Due to the large scale and complexity of modern software systems, the volume of logs has reached an unprecedented level. Consequently, for log-based anomaly detection, conventional manual inspection methods or even traditional machine learning-based methods become impractical, which serve as a catalyst for the rapid development of deep learning-based solutions. However, there is currently a lack of rigorous comparison among the representative log-based anomaly detectors that resort to neural networks. Moreover, the re-implementation process demands non-trivial efforts, and bias can be easily introduced. To better understand the characteristics of different anomaly detectors, in this paper, we provide a comprehensive review and evaluation of five popular neural networks used by six state-of-the-art methods. Particularly, four of the selected methods are unsupervised, and the remaining two are supervised. These methods are evaluated with two publicly available log datasets, which contain nearly 16 million log messages and 0.4 million anomaly instances in total. We believe our work can serve as a basis in this field and contribute to future academic research and industrial applications.},
  address       = {[Erscheinungsort nicht ermittelbar]},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the 2018 ACM Conference on Computer and Communications Security},
  copyright     = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  doi           = {10.48550/ARXIV.2107.05908},
  eprint        = {2107.05908},
  isbn          = {979-8400717529},
  keywords      = {Software Engineering (cs.SE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  ppn_gvk       = {1913850374},
  primaryclass  = {cs.SE},
  publisher     = {Association for Computing Machinery (ACM)},
  series        = {ACM Digital Library},
  url           = {https://arxiv.org/abs/2107.05908},
}

@Article{Lee2020a,
  author    = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal   = {Bioinformatics},
  title     = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  year      = {2020},
  issn      = {1367-4811},
  month     = sep,
  number    = {4},
  pages     = {1234, 1240},
  volume    = {36},
  abstract  = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
  doi       = {10.1093/bioinformatics/btz682},
  editor    = {Wren, Jonathan},
  eprint    = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics_36_4_1234.pdf},
  publisher = {Oxford University Press (OUP)},
  url       = {https://doi.org/10.1093/bioinformatics/btz682},
}

@Article{Beyer2025a,
  author    = {Beyer, Alexy and Alexy, Luca},
  title     = {An Image Equals 16x16 Words: Scaling Image Recogni:on with Transformers},
  year      = {2025},
  abstract  = {While Transformers have become the dominant architecture for natural language processing, their adop?on in computer vision has been rela?vely limited. Typically, aAen?on mechanisms are combined with convolu?onal networks or used to replace specific components while retaining the overall CNN structure. However, we demonstrate that CNNs are not essen?al, as a pure Transformer model applied directly to sequences of image patches can achieve strong performance in image classifica?on. When pre-trained on large datasets and fine-tuned on mid-sized or smaller benchmarks (such as ImageNet, CIFAR-100, and VTAB), the Vision Transformer (ViT) achieves compe??ve results compare},
  doi       = {10.2139/ssrn.5180447},
  publisher = {Elsevier BV},
}

@Comment{jabref-meta: databaseType:bibtex;}
