@Article{radford2021learning,
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  journal       = {International Conference on Machine Learning (ICML)},
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  year          = {2021},
  note          = {arXiv:2103.00020},
  abstract      = {State-Of-The-Art Computer Vision Systems Are Trained To Predict A Fixed Set Of Predetermined Object Categories. This Restricted Form Of Supervision Limits Their Generality And Usability Since Additional Labeled Data Is Needed To Specify Any Other Visual Concept. Learning Directly From Raw Text About Images Is A Promising Alternative Which Leverages A Much Broader Source Of Supervision. We Demonstrate That The Simple Pre-Training Task Of Predicting Which Caption Goes With Which Image Is An Efficient And Scalable Way To Learn Sota Image Representations From Scratch On A Dataset Of 400 Million (image, Text) Pairs Collected From The Internet. After Pre-Training, Natural Language Is Used To Reference Learned Visual Concepts (or Describe New Ones) Enabling Zero-Shot Transfer Of The Model To Downstream Tasks. We Study The Performance Of This Approach By Benchmarking On Over 30 Different Existing Computer Vision Datasets, Spanning Tasks Such As Ocr, Action Recognition In Videos, Geo-Localization, And Many Types Of Fine-Grained Object Classification. The Model Transfers Non-Trivially To Most Tasks And Is Often Competitive With A Fully Supervised Baseline Without The Need For Any Dataset Specific Training. For Instance, We Match The Accuracy Of The Original Resnet-50 On Imagenet Zero-Shot Without Needing To Use Any Of The 1.28 Million Training Examples It Was Trained On.},
  comment-paulo = {We Release Our Code And Pre-Trained Model Weights At This Https Url. : https://github.com/OpenAI/CLIP},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2103.00020},
  file          = {:New folder/2103.00020v1.pdf:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2103.00020},
}

@Article{gao2021making,
  author    = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal   = {ACL},
  title     = {Making Pre-trained Language Models Better Few-shot Learners},
  year      = {2020},
  note      = {arXiv:2012.15723},
  abstract  = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2012.15723},
  file      = {:New folder/2012.15723v2.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2012.15723},
}

@Article{zhao2021calibrate,
  author    = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal   = {ICLR},
  title     = {Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  year      = {2021},
  note      = {arxiv.org/abs/2102.09690},
  abstract  = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2102.09690},
  file      = {:New folder/2102.09690v2.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2102.09690},
}

@Article{liu2022wanli,
  author    = {Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  journal   = {arXiv preprint arXiv:2201.05955},
  title     = {WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},
  year      = {2022},
  note      = {arXiv:2201.05955},
  abstract  = {A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2201.05955},
  file      = {:New folder/2201.05955v5.pdf:PDF:https\://arxiv.org/abs/2201.05955},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2201.05955},
}

@InProceedings{bommasani2021opportunities,
  author        = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  booktitle     = {Stanford Center for Research on Foundation Models},
  title         = {On the Opportunities and Risks of Foundation Models},
  year          = {2021},
  note          = {arXiv:2108.07258},
  publisher     = {arXiv},
  abstract      = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  comment-paulo = {Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Report page with citation guidelines: this https URL: https://crfm.stanford.edu/report.html},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2108.07258},
  file          = {:New folder/2108.07258v3.pdf:PDF},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  url           = {https://arxiv.org/abs/2108.07258},
}

@Article{ganguli2023reducing,
  author    = {Deep Ganguli and Peter Hase and others},
  journal   = {arXiv preprint arXiv:2305.16328},
  title     = {Semantic Composition in Visually Grounded Language Models},
  year      = {2023},
  note      = {arXiv:2305.16328},
  abstract  = {What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically & relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether & how models compose visually grounded semantics, and how we might improve their ability to do so.
Specifically, we introduce 
1) WinogroundVQA, a new compositional visual question answering benchmark, 
2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 
3) Causal Tracing for Image Captioning Models to locate neural representations vital for vision-language composition, 
4) Syntactic MeanPool to inject a compositional inductive bias into sentence embeddings, and 
5) Cross-modal Attention Congruence Regularization, a self-supervised objective function for vision-language relation alignment. 
We close by discussing connections of our work to neuroscience, psycholinguistics, formal semantics, and philosophy.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2305.16328},
  file      = {:New folder/2305.16328v1.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2305.16328},
}

@Article{ouyang2022training,
  author    = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  journal   = {NeurIPS},
  title     = {Training language models to follow instructions with human feedback},
  year      = {2022},
  note      = {arXiv:2203.02155},
  abstract  = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2203.02155},
  file      = {:New folder/2203.02155v1.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2203.02155},
}

@Article{perez2022discovering,
  author    = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Discovering Language Model Behaviors with Model-Written Evaluations},
  year      = {2022},
  note      = {arXiv:2212.09251},
  abstract  = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2212.09251},
  file      = {:New folder/2212.09251v1.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2212.09251},
}

@InProceedings{Rabiner1989a,
  author         = {Rabiner, L. R.},
  title          = {A tutorial on hidden Markov models and selected applications in speech recognition},
  year           = {1989},
  number         = {2},
  organization   = {Proceedings of the IEEE,},
  pages          = {257–286},
  publisher      = {IEEE},
  volume         = {77},
  doi            = {10.1109/5.18626},
  issn           = {0018-9219},
  journal        = {Proceedings of the IEEE},
  msbib-accessed = {2025-01-07},
  url            = {https://doi.org/10.1109/5.18626/},
}

@Article{Devlin2018a,
  author   = {Devlin, J. and Chang, M. W. and Lee, K. and Toutanova, K.},
  journal  = {NAACL-HLT},
  title    = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year     = {2018},
  note     = {arXiv preprint arXiv:1810.04805.},
  language = {english},
}

@Article{Huang2015a,
  author   = {Huang, Z. and Xu, W. and Yu, K.},
  journal  = {preprint},
  title    = {Bidirectional LSTM-CRF Models for Sequence Tagging},
  year     = {2015},
  note     = {arXiv preprint arXiv:1508.01991.},
  language = {english},
}

@InProceedings{Lafferty2001a,
  author       = {Lafferty, McCallum A. and Pereira, F. C. N.},
  booktitle    = {Proceedings of the Eighteenth International Conference on Machine Learning},
  title        = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  year         = {2001},
  organization = {Proceedings of the Eighteenth International Conference on Machine Learning},
  language     = {english},
}

@Article{Mikolov2013a,
  author   = {Mikolov, Chen K. Corrado G. and Dean, J.},
  title    = {Efficient estimation of word representations in vector space},
  year     = {2013},
  note     = {arXiv preprint arXiv:1301.3781.},
  language = {english},
}

@InProceedings{Pennington2014a,
  author       = {Pennington, J. and Socher, R. and Manning, C. D.},
  booktitle    = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  title        = {Glove: Global vectors for word representation},
  year         = {2014},
  organization = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages        = {1532--1543},
  language     = {english},
}

@Article{McCallum2005a,
  author   = {McCallum, A.},
  journal  = {Queue},
  title    = {Information extraction: Distilling structured data from unstructured text},
  year     = {2005},
  pages    = {48–57},
  volume   = {3},
  language = {english},
}

@Article{Lample2016a,
  author   = {Lample, G. and Ballesteros, M. and Subramanian, S. and Kawakami, K. and Dyer, C.},
  journal  = {preprint},
  title    = {Neural architectures for named entity recognition},
  year     = {2016},
  note     = {arXiv preprint arXiv:1603.01360.},
  language = {english},
}

@Article{Pang2008a,
  author   = {Pang, B. and Lee, L.},
  journal  = {Foundations and Trends® in Information Retrieval},
  title    = {Opinion mining and sentiment analysis},
  year     = {2008},
  pages    = {1–135},
  volume   = {2},
  language = {english},
}

@InProceedings{Manning2011a,
  author       = {Manning, C. D.},
  booktitle    = {Computational Linguistics and Intelligent Text Processing},
  title        = {Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?},
  year         = {2011},
  organization = {Computational Linguistics and Intelligent Text Processing},
  language     = {english},
}

@Article{Honnibal2017a,
  author   = {Honnibal, M. and Montani, I.},
  journal  = {preprint},
  title    = {spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing},
  year     = {2017},
  note     = {To appear.},
  language = {english},
}

@InProceedings{Jagannatha2016a,
  author       = {Jagannatha, A. and Yu, H.},
  booktitle    = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  title        = {Structured prediction models for RNN based sequence labeling in clinical text},
  year         = {2016},
  organization = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  pages        = {856–865},
  language     = {english},
}

@Article{Manning2014a,
  author   = {Manning, C. D.},
  journal  = {Association for Computational Linguistics (ACL) System Demonstrations},
  title    = {The Stanford CoreNLP Natural Language Processing Toolkit},
  year     = {2014},
  pages    = {55–60},
  language = {english},
}

@Comment{jabref-meta: databaseType:bibtex;}
