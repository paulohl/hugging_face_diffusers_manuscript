@article{goldberg2016a,
  author = {Goldberg, Y.},
  date = {2016},
  title = {A Primer on Neural Network Models for Natural Language Processing},
  volume = {57},
  pages = {345–420},
  language = {en},
  journal = {Journal of Artificial Intelligence Research}
}

@Article{mikolov2013a,
  author   = {Mikolov, T. and Sutskever, I. and Chen, K. and Corrado, G.S. and Dean, J.},
  journal  = {Advances in Neural Information Processing Systems (NIPS)},
  title    = {Distributed Representations of Words and Phrases and their Compositionality},
  year     = {2013},
  pages    = {3111–3119},
  date     = {2013},
  language = {en},
}

@Proceedings{pennington2014a,
  title     = {GloVe: Global Vectors for Word Representation},
  author    = {Pennington, J. and Socher, R. and Manning, C.D.},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532-1543},
  date      = {2014},
  language  = {en},
}

@Book{jurafsky2019a,
  author    = {Jurafsky, D. and Martin, J.H.},
  publisher = {Pearson},
  title     = {Speech and Language Processing},
  year      = {2019},
  edition   = {3rd},
  date      = {2019},
  language  = {en},
}

@Article{collobert2011a,
  author        = {Collobert, R. and Weston, J. and Bottou, L. and Karlen, M. and Kavukcuoglu, K. and Kuksa, P.},
  journal       = {Journal of Machine Learning Research},
  title         = {Natural Language Processing (Almost) from Scratch},
  pages         = {2493–2537},
  volume        = {12},
  archiveprefix = {arXiv},
  date          = {2011},
  eprint        = {1103.0398},
  language      = {en},
  primaryclass  = {cs.LG},
}

@Article{mikolov2013b,
  author   = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
  journal  = {preprint},
  title    = {Efficient Estimation of Word Representations in Vector Space},
  year     = {2013},
  note     = {arXiv preprint arXiv:1301.3781.},
  arxiv    = {1301.3781},
  date     = {2013},
  language = {es},
}

@Article{Bojanowski_2017,
  author        = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal       = {Transactions of the Association for Computational Linguistics},
  title         = {Enriching Word Vectors with Subword Information},
  year          = {2017},
  issn          = {2307-387X},
  month         = dec,
  pages         = {135–146},
  volume        = {5},
  abstract      = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2017},
  doi           = {10.1162/tacl_a_00051},
  eprint        = {1607.04606},
  eprinttype    = {arXiv},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  language      = {en},
  publisher     = {MIT Press},
  timestamp     = {Mon, 28 Dec 2020 11:31:02 +0100},
  url           = {https://www.semanticscholar.org/paper/e2dba792360873aef125572812f3673b1a85d850},
  venue         = {Transactions of the Association for Computational Linguistics},
}

@InProceedings{bahdanau2015a,
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle     = {International Conference on Learning Representations},
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year          = {2015},
  publisher     = {arXiv},
  abstract      = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  arxiv         = {1409.0473},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2015},
  doi           = {10.48550/ARXIV.1409.0473},
  eprint        = {1409.0473},
  journal       = {International Conference on Learning Representations},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  language      = {en},
  url           = {https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5},
  venue         = {International Conference on Learning Representations},
}

@Article{yang2019a,
  author        = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal       = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year          = {2017},
  pages         = {5754–5764},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2017},
  doi           = {10.48550/ARXIV.1607.04606},
  eprint        = {1607.04606},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  language      = {en},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1607.04606},
}

@Article{radford2018a,
  author   = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
  journal  = {Open AI Research},
  title    = {Improving Language Understanding by Generative Pre-training},
  year     = {2018},
  note     = {Retrieved from},
  date     = {2018},
  language = {en},
  url      = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
}

@Article{raffel2019a,
  author   = {Raffel, C. and Shazeer, N. and Roberts, A. and Lee, K. and Narang, S. and Matena, M. and Liu, P.J.},
  journal  = {preprint},
  title    = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.},
  year     = {2019},
  note     = {arXiv preprint arXiv:1910.10683.},
  arxiv    = {1910.10683},
  date     = {2019},
  language = {en},
}

@Article{brown2020a,
  author   = {Brown, T.B. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Amodei, D.},
  journal  = {preprint},
  title    = {Language Models are Few-Shot Learners.},
  year     = {2020},
  note     = {arXiv preprint arXiv:2005.14165.},
  arxiv    = {2005.14165},
  date     = {2020},
  language = {sq},
}

@Article{vaswani2017a,
  author    = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A.N. and Polosukhin, I.},
  title     = {Attention is All You Need},
  pages     = {5998–6008},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  date      = {2017},
  language  = {en},
}

@Article{liu2020a,
  author   = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Stoyanov, V.},
  journal  = {preprint},
  title    = {RoBERTa: A Robustly Optimized BERT Pretraining Approach.},
  year     = {2020},
  note     = {arXiv preprint arXiv:1907.11692.},
  arxiv    = {1907.11692},
  date     = {2020},
  language = {hu},
}

@Article{dosovitskiy2020a,
  author    = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Houlsby, N.},
  journal   = {preprint},
  title     = {An Image Equals 16x16 Words: Scaling Image Recogni:on with Transformers},
  year      = {2020},
  note      = {arXiv preprint arXiv:2010.11929.},
  arxiv     = {2010.11929},
  date      = {2020},
  doi       = {10.2139/ssrn.5180447},
  eprint    = {2010.11929},
  language  = {en},
  publisher = {Elsevier BV},
  url       = {https://arxiv.org/abs/2010.11929},
}

@InProceedings{Pennington2014,
  author = {Pennington, J. and Socher, R. and Manning, C. D.},
  title  = {Glove: Global vectors for word representation},
  year   = {2014},
  pages  = {1532-1543},
}

@Article{Pang2008,
  author  = {Pang, B. and Lee, L.},
  journal = {Foundations and Trends® in Information Retrieval},
  title   = {Opinion mining and sentiment analysis},
  year    = {2008},
  pages   = {1–135},
  volume  = {2},
}

@Article{Mikolov2013,
  author = {Mikolov, Chen K. Corrado G. and Dean, J.},
  title  = {Efficient estimation of word representations in vector space},
  year   = {2013},
}

@Article{McCallum2005,
  author  = {McCallum, A.},
  journal = {Queue},
  title   = {Information extraction: Distilling structured data from unstructured text},
  year    = {2005},
  pages   = {48–57},
  volume  = {3},
}

@Article{Manning2014,
  author  = {Manning, C. D.},
  journal = {Association for Computational Linguistics (ACL) System Demonstrations},
  title   = {The Stanford CoreNLP Natural Language Processing Toolkit},
  year    = {2014},
  pages   = {55–60},
}

@InProceedings{Manning2011,
  author = {Manning, C. D.},
  title  = {Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?},
  year   = {2011},
}

@Article{Lample2016,
  author  = {Lample, G. and Ballesteros, M. and Subramanian, S. and Kawakami, K. and Dyer, C.},
  journal = {preprint},
  title   = {Neural architectures for named entity recognition},
  year    = {2016},
}

@InProceedings{Lafferty2001,
  author = {Lafferty, McCallum A. and Pereira, F. C. N.},
  title  = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  year   = {2001},
}

@InProceedings{Jagannatha2016,
  author = {Jagannatha, A. and Yu, H.},
  title  = {Structured prediction models for RNN based sequence labeling in clinical text},
  year   = {2016},
  pages  = {856–865},
}

@Article{Huang2015,
  author  = {Huang, Z. and Xu, W. and Yu, K.},
  journal = {preprint},
  title   = {Bidirectional LSTM-CRF Models for Sequence Tagging},
  year    = {2015},
}

@Article{Honnibal2017,
  author  = {Honnibal, M. and Montani, I.},
  journal = {preprint},
  title   = {spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing},
  year    = {2017},
}

@InProceedings{Rabiner1989,
  author    = {Rabiner, L. R.},
  title     = {A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
  year      = {1989},
  pages     = {257–286},
  publisher = {IEEE},
  volume    = {77},
}

@InProceedings{Brown2020a,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Language Models are Few-Shot Learners},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
}

@Article{Devlin2018,
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal       = {NAACL-HLT},
  title         = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year          = {2018},
  pages         = {4171–4186},
  volume        = {1},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  date          = {2019},
  doi           = {10.18653/v1/N19-1423},
  eprint        = {1810.04805v2},
  language      = {en},
  primaryclass  = {cs.CL},
  publisher     = {Long and Short Papers},
  url           = {https://arxiv.org/abs/1810.04805},
}

@Comment{jabref-meta: databaseType:bibtex;}
